{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbdacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4821865d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hereutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m puhti_files\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_basis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/../src/common_basis.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhereutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m here\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLAlchemyError, ArgumentError\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hereutil'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "sys.path.append(\"../\")\n",
    "from src import puhti_files\n",
    "from src.common_basis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b72d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = puhti_files.genre_data_to_pandas(data=\"dev\", add_labels=True, merge_ecco=True)\n",
    "df = df[~df['publication_year'].isna()]\n",
    "df['decade'] = df['publication_year'].apply(lambda x: int(str(int(x))[:-1]+'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d172f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e56be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['work_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b627178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['work_id']=='74-hamlet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic classifications, combining both sebastian and Eli analisis.\n",
    "# NA - > The topic only contains weird tokens, doesn't represent anything\n",
    "topics_names = {1720:{1:'Religion',2:'Goverment/Royal',3:'Romance',4:'Prayer/Bible',5:'Social',6:'Law',7:'Science',8:'Kings',9:'Verbs?/War?',10:'War?'},\n",
    "                1730:{1:'Religion',2:'Goverment/Kingdom',3:'Religion',4:'Social Concepts',5:'State',6:'Romance',7:'Nature',8:'Locations/People',9:'Kings',10:'Bigger Concepts'},\n",
    "                1740:{1:'Religion',2:'Romance',3:'Religion',4:'Philosophy',5:'War/Kingdom',6:'Romance',7:'Nature',8:'Law',9:'Royalty',10:'NA?'},\n",
    "                1750:{1:'Bible/Religion',2:'Romance/Religion',3:'Religion',4:'Nature',5:'Kingdom',6:'Mankind',7:'Kingdom',8:'Country/Issues',9:'NA',10:'NA'},\n",
    "                1760:{1:'Bible/Religion',2:'State Affairs',3:'Religion',4:'Kingdom',5:'Love/Family',6:'Government affairs',7:'Love/Religious Concepts',8:'Nature',9:'Philosophy',10:'NA'},\n",
    "                1770:{1:'Kingdom/War',2:'Law/Court',3:'Love/Religion',4:'Higher Power',5:'Nature',6:'Mankind',7:'Kingdom',8:'Love',9:'Church/Religion',10:'NA'},\n",
    "                1780:{1:'Prayer/Bible',2:'War',3:'Romance',4:'Mandkind',5:'Religion',6:'Law',7:'Trade/Economy',8:'Court/Kingdom',9:'Medicine',10:'NA'},\n",
    "                1790:{1:'Love',2:'Literature?',3:'Law',4:'Religion',5:'Parliament',6:'Mankind',7:'Romance',8:'Ingredients?',9:'Countries/Land/Economy',10:'NA'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame.from_dict(topics_names)\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_table('/scratch/project_2007227/genre_data/ecco_metadata.tsv')\n",
    "#df = df[~df['publication_year'].isna()]\n",
    "#df['decade'] = df['publication_year'].apply(lambda x: int(str(int(x))[:-1]+'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original visualization, just for one book. It must receive an ID that is a str\n",
    "def book_pattern_vis(id, chunksize=512, ybottom=0):\n",
    "    #We need to identify the age of the book to use the correct model. Extract year from metadata\n",
    "    id= str(id)\n",
    "    year = 1720\n",
    "    try:\n",
    "        year = df[df['document_id'] ==id]['publication_year'].values[0]\n",
    "    except:\n",
    "        print('Doc id not found')\n",
    "    year = int(year)\n",
    "    work_id = df[df['document_id'] ==id]['work_id'].values[0]\n",
    "    #if the book is from earlier era than 1720, gives erros since we don't have model from that era.\n",
    "    if year < 1720:\n",
    "        print('Publication year of the document is not included in the models, please try another document.')\n",
    "        return True\n",
    "    \n",
    "    #if the book is ok, we need to first preprocess the document for the model\n",
    "    \n",
    "    #First divide the text into chunks\n",
    "    with open('/scratch/project_2007227/genre_data/ecco_source/'+str(id)+'.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    num_lines = len(lines)\n",
    "    text = \" \".join(lines)\n",
    "    new_text = '\\n'.join([text[i:i+chunksize] for i in range(0, len(text), chunksize)])\n",
    "    \n",
    "    with open('/scratch/project_2007227/genre_data/chunk512/'+str(id)+'512.txt', 'w') as f:\n",
    "        f.write(new_text)\n",
    "              \n",
    "    text_file = '/scratch/project_2007227/genre_data/chunk512/'+str(id)+'512.txt'\n",
    "    documents = [line.strip() for line in open(text_file, encoding=\"utf-8\").readlines()[0:num_lines]]\n",
    "\n",
    "    stopwords = list(stop_words.words(\"english\"))\n",
    "\n",
    "    sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
    "    preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "    \n",
    "    \n",
    "    #tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "    #processed_data= tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)\n",
    "    \n",
    "    #Nextly we fetch the model that corresponds to the publication year\n",
    "    modelnumber = ((year // 10) % 10)-2\n",
    "    decade = int(str(int(year))[:-1]+'0')\n",
    "    with open('/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/tp_models/tp_model'+str(modelnumber)+'.pickle', 'rb') as tpmodel:\n",
    "        tpmodeli = pickle.load(tpmodel)\n",
    "\n",
    "    ctm = ZeroShotTM(bow_size=len(tpmodeli.vocab), contextual_size=768, n_components=10, num_epochs=20)\n",
    "    ctm.load(\"/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/ctm_models\"+str(modelnumber)+\"/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\",\n",
    "                                                                                                          epoch=19)\n",
    "    processed_data = tpmodeli.transform(preprocessed_documents)\n",
    "    pred_model = ctm.get_thetas(processed_data, n_samples=20) \n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(pred_model, '.')\n",
    "    \n",
    "    plt.ylim((ybottom,1))\n",
    "    plt.title('Sequential genre prediction for document '+str(id)+' in with 512 token chunks. The document is from the decade '+str(decade) + '. With work id ' + str(work_id))\n",
    "    plt.legend(topics_names[decade].values(),loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f088f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "book_pattern_vis('1299000200')\n",
    "#Currently the method doesn't support ids starting with 0, to address this feel free to modify str<->int in the code, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that given a decade, will visualize 6 samples of it.\n",
    "#If random_state is True, it will always use the same random state seed, which means that it will always return the same samples\n",
    "#To see different documents, use random_state=False, so it shows a completly random sample of 6 documents\n",
    "def book_pattern_vis_sample(decade, chunksize=512, ybottom=0,num_samples=6,random_state=True):\n",
    "    #We need to identify the age of the book to use the correct model. Extract year from metadata\n",
    "    if random_state:\n",
    "        samples = df[df['decade']==decade].sample(n = num_samples,random_state=423)\n",
    "    else:\n",
    "        samples = df[df['decade']==decade].sample(n = num_samples)\n",
    "\n",
    "    ids = samples['document_id'].tolist()\n",
    "    works_ids = samples['work_id'].tolist()\n",
    "    stopwords = list(stop_words.words(\"english\"))\n",
    "    \n",
    "    modelnumber = ((decade // 10) % 10)-2\n",
    "    #decade = int(str(int(year))[:-1]+'0')\n",
    "    with open('/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/tp_models/tp_model'+str(modelnumber)+'.pickle', 'rb') as tpmodel:\n",
    "        tpmodeli = pickle.load(tpmodel)\n",
    "\n",
    "    ctm = ZeroShotTM(bow_size=len(tpmodeli.vocab), contextual_size=768, n_components=10, num_epochs=20)\n",
    "    ctm.load(\"/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/ctm_models\"+str(modelnumber)+\"/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\",\n",
    "                                                                                                          epoch=19)\n",
    "    \n",
    "    #First divide the text into chunks\n",
    "    documents = []\n",
    "    for i in ids:\n",
    "        len_id = len(str(i))\n",
    "        if len_id<10:\n",
    "            i = '0'*(10 - len_id) + str(i)\n",
    "        else:\n",
    "            i = str(i)\n",
    "        with open('/scratch/project_2007227/genre_data/ecco_source/'+str(i)+'.txt', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "            text = \" \".join(lines)\n",
    "            new_text = '\\n'.join([text[i:i+chunksize] for i in range(0, len(text), chunksize)])\n",
    "        with open('/scratch/project_2007227/genre_data/chunk512/'+str(i)+'512.txt', 'w') as f:\n",
    "            f.write(new_text)\n",
    "        text_file = '/scratch/project_2007227/genre_data/chunk512/'+str(i)+'512.txt'\n",
    "        documents.append([line.strip() for line in open(text_file, encoding=\"utf-8\").readlines()[0:num_lines]])\n",
    "    thetas = []\n",
    "    for i in range(len(documents)):\n",
    "        sp = WhiteSpacePreprocessingStopwords(documents[i], stopwords_list=stopwords)\n",
    "        preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "        processed_data = tpmodeli.transform(preprocessed_documents)\n",
    "        pred_model = ctm.get_thetas(processed_data, n_samples=20) \n",
    "        thetas.append(pred_model)\n",
    "    \n",
    "    #tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "    #processed_data= tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)\n",
    "    \n",
    "    #Nextly we fetch the model that corresponds to the publication year\n",
    "\n",
    "    figure, axis = plt.subplots(2, 3, figsize=(16,12),squeeze=False)\n",
    "    for i in range(len(thetas)):\n",
    "        axis[i%2, i%3].plot(thetas[i], '.')\n",
    "        axis[i%2, i%3].axis(ymin=ybottom,ymax=1)\n",
    "        axis[i%2, i%3].set_title(f\"D {ids[i]} - work id {str(works_ids[i])}\")\n",
    "        \n",
    "    figure.legend(topics_names[decade].values(),loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f22644",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_pattern_vis_sample(1720,random_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that uses pre_loaded test datasets to visualize. The problem is that it can only use the chunks, \n",
    "#but it doesn't have the original document\n",
    "def test_loaded_sets(decade):\n",
    "    modelnumber = ((decade // 10) % 10)-2\n",
    "    test_sets = []\n",
    "    with open('/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/tp_models/tp_model'+str(modelnumber)+'.pickle', 'rb') as tpmodel:\n",
    "        tpmodeli = pickle.load(tpmodel)\n",
    "\n",
    "    ctm = ZeroShotTM(bow_size=len(tpmodeli.vocab), contextual_size=768, n_components=10, num_epochs=20)\n",
    "    ctm.load(\"/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/ctm_models\"+str(modelnumber)+\"/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\",\n",
    "                                                                                                          epoch=19)\n",
    "\n",
    "    with open('/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/Test_sets/test_set'+str(modelnumber)+'.pickle', 'rb') as dataset:\n",
    "        dataseti = pickle.load(dataset)\n",
    "        test_sets.append(dataseti)\n",
    "\n",
    "    thetas = []\n",
    "    for i in test_sets:\n",
    "        pred_model = ctm.get_thetas(i, n_samples=20)\n",
    "        thetas.append(pred_model)\n",
    "\n",
    "    thetas = np.array(thetas)\n",
    "    thetas_ov = np.mean(thetas,axis=1)\n",
    "    thetas_ov = np.squeeze(thetas)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(thetas_ov, '.')\n",
    "    plt.ylim((0,1))\n",
    "    plt.title('Topic prediction for test set with  '+str(thetas_ov.shape[0])+' chunks. The documents are from the decade '+str(decade))\n",
    "    plt.legend(topics_names[decade].values(),loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaded_sets(1760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaded_sets(1780)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f95e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaded_sets(1720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that will sample 6 documents from the given decade and genre.\n",
    "#The decade must be between 1720 and 1790, and the genre must be the name of the genre, not the number.\n",
    "#If random_state is True, it will always use the same random state seed, which means that it will always return the same samples\n",
    "#To see different documents, use random_state=False, so it shows a completly random sample of 6 documents\n",
    "def sample_per_decade_and_genre(decade,genre, chunksize=512, ybottom=0,num_samples=6,random_state=True):\n",
    "    #We need to identify the age of the book to use the correct model. Extract year from metadata\n",
    "    if random_state:\n",
    "        filt = df[(df['decade']==decade) & (df['main_category_label']==genre)]\n",
    "        samples = filt.sample(n = min(num_samples,filt.shape[0]),random_state=423)\n",
    "    else:\n",
    "        filt = df[(df['decade']==decade) & (df['main_category_label']==genre)]\n",
    "        samples = filt.sample(n = min(num_samples,filt.shape[0]))\n",
    "\n",
    "    ids = samples['document_id'].tolist()\n",
    "    works_ids = samples['work_id'].tolist()\n",
    "    stopwords = list(stop_words.words(\"english\"))\n",
    "    \n",
    "    modelnumber = ((decade // 10) % 10)-2\n",
    "    #decade = int(str(int(year))[:-1]+'0')\n",
    "    with open('/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/tp_models/tp_model'+str(modelnumber)+'.pickle', 'rb') as tpmodel:\n",
    "        tpmodeli = pickle.load(tpmodel)\n",
    "\n",
    "    ctm = ZeroShotTM(bow_size=len(tpmodeli.vocab), contextual_size=768, n_components=10, num_epochs=20)\n",
    "    ctm.load(\"/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/ctm_models\"+str(modelnumber)+\"/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\",\n",
    "                                                                                                          epoch=19)\n",
    "    \n",
    "    #First divide the text into chunks\n",
    "    documents = []\n",
    "    for i in ids:\n",
    "        len_id = len(str(i))\n",
    "        if len_id<10:\n",
    "            i = '0'*(10 - len_id) + str(i)\n",
    "        else:\n",
    "            i = str(i)\n",
    "        with open('/scratch/project_2007227/genre_data/ecco_source/'+str(i)+'.txt', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "            text = \" \".join(lines)\n",
    "            new_text = '\\n'.join([text[i:i+chunksize] for i in range(0, len(text), chunksize)])\n",
    "        with open('/scratch/project_2007227/genre_data/chunk512/'+str(i)+'512.txt', 'w') as f:\n",
    "            f.write(new_text)\n",
    "        text_file = '/scratch/project_2007227/genre_data/chunk512/'+str(i)+'512.txt'\n",
    "        documents.append([line.strip() for line in open(text_file, encoding=\"utf-8\").readlines()[0:num_lines]])\n",
    "    thetas = []\n",
    "    for i in range(len(documents)):\n",
    "        sp = WhiteSpacePreprocessingStopwords(documents[i], stopwords_list=stopwords)\n",
    "        preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "        processed_data = tpmodeli.transform(preprocessed_documents)\n",
    "        pred_model = ctm.get_thetas(processed_data, n_samples=20) \n",
    "        thetas.append(pred_model)\n",
    "    \n",
    "    #tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "    #processed_data= tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)\n",
    "    \n",
    "    #Nextly we fetch the model that corresponds to the publication year\n",
    "\n",
    "    figure, axis = plt.subplots(2, 3, figsize=(16,12),squeeze=False)\n",
    "    for i in range(len(thetas)):\n",
    "        axis[i%2, i%3].plot(thetas[i], '.')\n",
    "        axis[i%2, i%3].axis(ymin=ybottom,ymax=1)\n",
    "        axis[i%2, i%3].set_title(f\"D {ids[i]} - work id {str(works_ids[i])[:100]}\")\n",
    "        \n",
    "    figure.legend(topics_names[decade].values(),loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the different labels that can be used in the following method\n",
    "df['main_category_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407981d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of use\n",
    "sample_per_decade_and_genre(1790,'Law')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_per_decade_and_genre(1760,'Politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a75318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_per_decade_and_genre(1740,'Philosophy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_per_decade_and_genre(1750,'Literature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that will sample 6 documents from the given decade and genre.\n",
    "#The decade must be between 1720 and 1790, and the genre must be the name of the genre, not the number.\n",
    "#If random_state is True, it will always use the same random state seed, which means that it will always return the same samples\n",
    "#To see different documents, use random_state=False, so it shows a completly random sample of 6 documents\n",
    "def sample_per_genre(genre, chunksize=512, ybottom=0,random_state=True):\n",
    "    #We need to identify the age of the book to use the correct model. Extract year from metadata\n",
    "    sampled_df = pd.DataFrame()\n",
    "    for i in range(1720,1800,10):\n",
    "        if random_state:\n",
    "            filt = df[(df['decade']==i) & (df['main_category_label']==genre)]\n",
    "            samples = filt.sample(n = 3,random_state=423)\n",
    "            sampled_df = pd.concat([sampled_df,samples])\n",
    "        else:\n",
    "            filt = df[(df['decade']==i) & (df['main_category_label']==genre)]\n",
    "            samples = filt.sample(n = 3)\n",
    "            sampled_df = pd.concat([sampled_df,samples])\n",
    "    tp_models = []\n",
    "    ctm_models = []\n",
    "    stopwords = list(stop_words.words(\"english\"))\n",
    "    thetas1 = []\n",
    "    for i in range(1720,1800,10):\n",
    "        modelnumber = ((i // 10) % 10)-2\n",
    "        #decade = int(str(int(year))[:-1]+'0')\n",
    "        with open('/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/tp_models/tp_model'+str(modelnumber)+'.pickle', 'rb') as tpmodel:\n",
    "            tpmodeli = pickle.load(tpmodel)\n",
    "        tp_models.append(tpmodeli)\n",
    "        ctm = ZeroShotTM(bow_size=len(tpmodeli.vocab), contextual_size=768, n_components=10, num_epochs=20)\n",
    "        ctm.load(\"/scratch/project_2007227/genre_data/Digital-Humanities-Project-Genres-2023/src/ctm_models\"+str(modelnumber)+\"/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\",\n",
    "                                                                                                              epoch=19)\n",
    "        ctm_models.append(ctm)\n",
    "    \n",
    "        #First divide the text into chunks\n",
    "        ids = sampled_df[sampled_df['decade']==i]['document_id'].tolist()\n",
    "        works_ids = sampled_df[sampled_df['decade']==i]['work_id'].tolist()\n",
    "        documents = []\n",
    "        for i in ids:\n",
    "            len_id = len(str(i))\n",
    "            if len_id<10:\n",
    "                i = '0'*(10 - len_id) + str(i)\n",
    "            else:\n",
    "                i = str(i)\n",
    "            with open('/scratch/project_2007227/genre_data/ecco_source/'+str(i)+'.txt', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                num_lines = len(lines)\n",
    "                text = \" \".join(lines)\n",
    "                new_text = '\\n'.join([text[i:i+chunksize] for i in range(0, len(text), chunksize)])\n",
    "            with open('/scratch/project_2007227/genre_data/chunk512/'+str(i)+'512.txt', 'w') as f:\n",
    "                f.write(new_text)\n",
    "            text_file = '/scratch/project_2007227/genre_data/chunk512/'+str(i)+'512.txt'\n",
    "            documents.append([line.strip() for line in open(text_file, encoding=\"utf-8\").readlines()[0:num_lines]])\n",
    "        thetas = []\n",
    "        for i in range(len(documents)):\n",
    "            sp = WhiteSpacePreprocessingStopwords(documents[i], stopwords_list=stopwords)\n",
    "            preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "            processed_data = tpmodeli.transform(preprocessed_documents)\n",
    "            pred_model = ctm.get_thetas(processed_data, n_samples=20) \n",
    "            thetas.append(pred_model)\n",
    "        thetas1.append(thetas)\n",
    "    \n",
    "    #tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "    #processed_data= tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)\n",
    "    \n",
    "    #Nextly we fetch the model that corresponds to the publication year\n",
    "    \n",
    "    figure, axis = plt.subplots(8, 3, figsize=(20,16),squeeze=False)\n",
    "    for i in range(len(thetas1)):\n",
    "        for j in range(len(thetas1[i])):\n",
    "            decade = 1720 + i*10\n",
    "            axis[i, j].plot(thetas1[i][j], '.')\n",
    "            axis[i, j].axis(ymin=ybottom,ymax=1)\n",
    "            axis[i, j].set_title(f\"Decade {decade}\")\n",
    "            if j == 2:\n",
    "                axis[i, j].legend(topics_names[decade].values(),loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21477bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_per_genre('Law')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62980cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
